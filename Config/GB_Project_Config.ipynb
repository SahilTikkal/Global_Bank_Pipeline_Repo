{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22b39a8d-4d26-4050-9159-79f6bcfb0464",
     "showTitle": true,
     "title": "Importing Libraries"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.streaming import *\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from urllib.parse import quote\n",
    "import pandas as pd\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f76b1f32-2c5f-46dc-a50a-f184b1f55d20",
     "showTitle": true,
     "title": "Customer Table Schema"
    }
   },
   "outputs": [],
   "source": [
    "# Define schema for Customers table\n",
    "customer_schema =StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"phone\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"credit_score\", IntegerType(), True),\n",
    "    StructField(\"join_date\", DateType(), True),\n",
    "    StructField(\"last_update\", TimestampType(), True),\n",
    "    StructField(\"customer_type\", StringType(), True)\n",
    "]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad5aefce-1bfc-417c-ac95-32882c209d5a",
     "showTitle": true,
     "title": "Branch Table Schema"
    }
   },
   "outputs": [],
   "source": [
    "# Define schema for Branch table\n",
    "branches_schema = StructType([\n",
    "    StructField(\"branch_id\", StringType(), True),\n",
    "    StructField(\"branch_name\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"timezone\", StringType(), True),\n",
    "    StructField(\"currency\", StringType(), True)\n",
    "]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd8acfaf-889a-47f1-bcfa-858864a143a2",
     "showTitle": true,
     "title": "Transaction Table Schema"
    }
   },
   "outputs": [],
   "source": [
    "# Define schema for transaction data\n",
    "transactions_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"branch_id\", StringType(), True),\n",
    "    StructField(\"channel\", StringType(), True),\n",
    "    StructField(\"transaction_type\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"currency\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4379bae0-a805-4920-9f9c-a2a9f7cc8908",
     "showTitle": true,
     "title": "Bronze config"
    }
   },
   "outputs": [],
   "source": [
    "## configuration for bronze file\n",
    "\n",
    "config = {\n",
    "  \"adls_connection\" :{\n",
    "    \"storage_account\" : \"mavericstoragecapstone\",\n",
    "    \"container_name\" : \"global-bank\",\n",
    "    \"access_key\" : \" NOT_allowed_on_GIThub \",\n",
    "    \"storage_account_conf_key\" : \"fs.azure.account.key.mavericstoragecapstone.dfs.core.windows.net\",\n",
    "    \"ingestion_location\" : \"abfss://global-bank@mavericstoragecapstone.dfs.core.windows.net/\"\n",
    "  },\n",
    "  \"paths\":{\n",
    "    \"customers\" : \"/Transactions/csv/Customers.csv\",\n",
    "    \"branch\" : \"/Transactions/csv/Branches.csv\",\n",
    "    \"transactions\" : \"/Transactions/Transactions/\"\n",
    "  },\n",
    "\n",
    "    \"cloudFiles\": {\n",
    "        \"format\": \"csv\",\n",
    "        \"header\": \"true\",\n",
    "        \"timestampFormat\": \"yyyy-MM-dd\",\n",
    "        \"schemaLocation\": \"dbfs:/FileStore/Streaming_Schema/Bronze/Transactions\",\n",
    "        \"inferColumnTypes\": \"true\"\n",
    "    },\n",
    "    \"delta\": {\n",
    "        \"checkpointLocation\": \"dbfs:/FileStore/Checkpoints/Bronze/Transactions\",\n",
    "        \"mergeSchema\": \"true\",\n",
    "        \"outputMode\": \"append\",\n",
    "        \"processingTime\": \"30 seconds\",\n",
    "        \"table\": \"hive_metastore.gb_bronze_schema.transactions_Streaming\"\n",
    "    }\n",
    "  \n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fb9056c-2141-4623-8cf4-22be3e8bc29f",
     "showTitle": true,
     "title": "FN: Cleaning Funtions"
    }
   },
   "outputs": [],
   "source": [
    "## Cleaning Funtions\n",
    "\n",
    "# 1. Remove duplicates\n",
    "def remove_duplicates(df: DataFrame, primary_keys: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Remove duplicate rows based on primary keys.\n",
    "    \"\"\"\n",
    "    return df.dropDuplicates(primary_keys)\n",
    "\n",
    "# 2. Handling Missing Values\n",
    "def handle_missing_values(df: DataFrame, fill_values: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Fill missing values with specified values.\n",
    "    \"\"\"\n",
    "    df1 = df.na.fill(fill_values)\n",
    "    return df1.dropna()\n",
    "\n",
    "# 3. Validate Data Type using schema\n",
    "def validate_data_types(df: DataFrame, schema: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure columns have the correct data types.\n",
    "    \"\"\"\n",
    "    for col_name, col_type in schema.items():\n",
    "        df = df.withColumn(col_name, col(col_name).cast(col_type))\n",
    "    return df\n",
    "\n",
    "# 4. Trimming White spaces\n",
    "def trim_whitespaces(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Remove leading and trailing whitespaces from string columns.\n",
    "    \"\"\"\n",
    "    string_cols = [field.name for field in df.schema.fields if isinstance(field.dataType, StringType)]\n",
    "    for col_name in string_cols:\n",
    "        df = df.withColumn(col_name, trim(col(col_name)))\n",
    "    return df\n",
    "\n",
    "# 5. Remove Invalid Values\n",
    "def remove_invalid_values(df: DataFrame, invalid_values: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Remove rows with invalid or unrealistic values.\n",
    "    \"\"\"\n",
    "    for col_name, value_range in invalid_values.items():\n",
    "        df = df.filter((col(col_name) >= value_range[0]) & (col(col_name) <= value_range[1]))\n",
    "    return df\n",
    "\n",
    "# 6. Enforce Uniqueness on the primary key columns\n",
    "def enforce_uniqueness(df: DataFrame, unique_cols: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Enforce uniqueness constraints on key columns.\n",
    "    \"\"\"\n",
    "    return df.dropDuplicates(unique_cols)\n",
    "\n",
    "# 7. Capitalize string data to make data homogenous\n",
    "def capitalize_string_data(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure that string data such as names are properly capitalized.\n",
    "    \"\"\"\n",
    "    string_cols = [field.name for field in df.schema.fields if isinstance(field.dataType, StringType)]\n",
    "    for col_name in string_cols:\n",
    "        df = df.withColumn(col_name, upper(col(col_name)))\n",
    "    return df\n",
    "    \n",
    "# 8. Flag anomalies using conditions \n",
    "def flag_anomalies(df: DataFrame, anomaly_conditions: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Flag rows with anomalies for further inspection.\n",
    "    \"\"\"\n",
    "    for col_name, condition in anomaly_conditions.items():\n",
    "        df = df.withColumn(\"anomaly_flag\", when(condition, True).otherwise(col(\"anomaly_flag\")))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40b99cf8-da01-41cc-9596-b22d09a52439",
     "showTitle": true,
     "title": "FN: Data Summery Functions"
    }
   },
   "outputs": [],
   "source": [
    "# FN: Data Summery Functions\n",
    "\n",
    "def data_summary(df: DataFrame):\n",
    "    summary = {}\n",
    "\n",
    "    # Count of total rows\n",
    "    total_count = df.count()\n",
    "    summary[\"total_count\"] = total_count\n",
    "\n",
    "    # Loop through each column and gather summary statistics\n",
    "    for col_name in df.columns:\n",
    "        summary[col_name] = {}\n",
    "\n",
    "        # Count of non-null values\n",
    "        non_null_count = df.filter(col(col_name).isNotNull()).count()\n",
    "        summary[col_name][\"non_null_count\"] = non_null_count\n",
    "\n",
    "        # Count of unique values\n",
    "        unique_count = df.select(countDistinct(col(col_name)).alias(\"count\")).collect()[0][\"count\"]\n",
    "        summary[col_name][\"unique_count\"] = unique_count\n",
    "\n",
    "        # If the column is numeric, get additional statistics\n",
    "        if isinstance(df.schema[col_name].dataType, NumericType):\n",
    "            numeric_summary = df.select(\n",
    "                mean(col(col_name)).alias(\"mean\"),\n",
    "                stddev(col(col_name)).alias(\"stddev\"),\n",
    "                min(col(col_name)).alias(\"min\"),\n",
    "                max(col(col_name)).alias(\"max\")\n",
    "            ).collect()[0]\n",
    "            summary[col_name].update({\n",
    "                \"mean\": numeric_summary[\"mean\"],\n",
    "                \"stddev\": numeric_summary[\"stddev\"],\n",
    "                \"min\": numeric_summary[\"min\"],\n",
    "                \"max\": numeric_summary[\"max\"]\n",
    "            })\n",
    "\n",
    "    # Print summary in a readable format\n",
    "    print(f\"Total Rows: {total_count}\\n\")\n",
    "    for col_name, stats in summary.items():\n",
    "        if col_name != \"total_count\":\n",
    "            print(\"==o\"*10)\n",
    "            print(f\"Column: {col_name}\")\n",
    "            print(f\"  Non-Null Count: {stats['non_null_count']}\")\n",
    "            print(f\"  Null count:  {total_count - (stats['non_null_count'])}\")\n",
    "            print(f\"  Unique Count: {stats['unique_count']}\")\n",
    "            if \"mean\" in stats:\n",
    "                print(f\"  Mean: {stats['mean']}\")\n",
    "                print(f\"  StdDev: {stats['stddev']}\")\n",
    "                print(f\"  Min: {stats['min']}\")\n",
    "                print(f\"  Max: {stats['max']}\")\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93cf64ae-af1d-440f-9309-1392ffc16cf2",
     "showTitle": true,
     "title": "FN: Cleaning duplicates in Transactions"
    }
   },
   "outputs": [],
   "source": [
    "def clean_duplicate_transactions(df: DataFrame, primary_keys: list, order_by_col: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Remove duplicate rows based on primary keys, keeping the last instance of duplicates.\n",
    "    \"\"\"\n",
    "    # Define a window specification to partition by primary keys and order by order_by_col in descending order\n",
    "    window_spec = Window.partitionBy(*primary_keys).orderBy(col(order_by_col).desc())\n",
    "\n",
    "    # Add a row number column based on the window specification\n",
    "    df_with_row_num = df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "\n",
    "    # Filter rows to keep only the last instance (row_num == 1) and drop the row_num column\n",
    "    df_deduplicated = df_with_row_num.filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
    "    \n",
    "    return df_deduplicated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d18a9d6d-15f7-43ab-b4fd-35e80b04d3af",
     "showTitle": true,
     "title": "FN: Format Dates and Timestamps"
    }
   },
   "outputs": [],
   "source": [
    "# FN: Format Dates and Timestamps\n",
    "def format_dates(df: DataFrame, date_columns: list = [], timestamp_columns: list = []) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Formats specified columns in a DataFrame as dates or timestamps.\n",
    "    \"\"\"\n",
    "    for col_name in date_columns:\n",
    "        if col_name in df.columns:\n",
    "            try:\n",
    "                df = df.withColumn(col_name, to_date(col(col_name)))\n",
    "            except Exception as e:\n",
    "                print(f\"Error formatting column {col_name} to date: {e}\")\n",
    "        else:\n",
    "            print(f\"Column {col_name} does not exist in DataFrame.\")\n",
    "\n",
    "    for col_name in timestamp_columns:\n",
    "        if col_name in df.columns:\n",
    "            try:\n",
    "                df = df.withColumn(col_name, to_timestamp(col(col_name)))\n",
    "                df = df.withColumn(col_name, date_format(col(col_name), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "            except Exception as e:\n",
    "                print(f\"Error formatting column {col_name} to timestamp: {e}\")\n",
    "        else:\n",
    "            print(f\"Column {col_name} does not exist in DataFrame.\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6e30765-e8a2-45e7-ba85-b41a03a107ee",
     "showTitle": true,
     "title": "FN: Extracting dates functions"
    }
   },
   "outputs": [],
   "source": [
    "# FN: Extracting dates functions\n",
    "\n",
    "def extract_date_parts(df: DataFrame, date_column: str, Year: bool = True, Month: bool = False, Day: bool = False) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts parts of a date from a specified column in a DataFrame and adds them as new columns.\n",
    "    \"\"\"\n",
    "    if date_column not in df.columns:\n",
    "        raise ValueError(f\"Column {date_column} does not exist in DataFrame.\")\n",
    "    \n",
    "    try:\n",
    "        if Year:\n",
    "            df = df.withColumn(\"year\", year(col(date_column)))\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting year from column {date_column}: {e}\")\n",
    "    \n",
    "    try:\n",
    "        if Month:\n",
    "            df = df.withColumn(\"month\", month(col(date_column)))\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting month from column {date_column}: {e}\")\n",
    "    \n",
    "    try:\n",
    "        if Day:\n",
    "            df = df.withColumn(\"day\", dayofmonth(col(date_column)))\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting day from column {date_column}: {e}\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6b563f5-4a43-4800-866f-d8218bd02d5c",
     "showTitle": true,
     "title": "FN: streaming_data_summary"
    }
   },
   "outputs": [],
   "source": [
    "# FN: streaming_data_summary\n",
    "\n",
    "def streaming_data_summary(df: DataFrame):\n",
    "    try:\n",
    "        # Check if DataFrame is streaming\n",
    "        if not df.isStreaming:\n",
    "            raise ValueError(\"The provided DataFrame is not a streaming DataFrame.\")\n",
    "\n",
    "        # Count of total rows (processed in streaming batches)\n",
    "        total_count_query = df.groupBy().agg(count(\"*\").alias(\"total_count\")) \\\n",
    "            .writeStream \\\n",
    "            .outputMode(\"complete\") \\\n",
    "            .format(\"console\") \\\n",
    "            .start()\n",
    "\n",
    "        for col_name in df.columns:\n",
    "            # Calculate non-null count and unique count for each column\n",
    "            col_summary_query = df.groupBy().agg(\n",
    "                count(col(col_name)).alias(f\"{col_name}_non_null_count\"),\n",
    "                approx_count_distinct(col(col_name)).alias(f\"{col_name}_unique_count\")\n",
    "            ).writeStream \\\n",
    "             .outputMode(\"complete\") \\\n",
    "             .format(\"console\") \\\n",
    "             .start()\n",
    "\n",
    "            col_summary_query.awaitTermination()\n",
    "\n",
    "            # If the column is numeric, calculate additional statistics\n",
    "            if isinstance(df.schema[col_name].dataType, NumericType):\n",
    "                numeric_summary_query = df.groupBy().agg(\n",
    "                    mean(col(col_name)).alias(f\"{col_name}_mean\"),\n",
    "                    stddev(col(col_name)).alias(f\"{col_name}_stddev\"),\n",
    "                    min(col(col_name)).alias(f\"{col_name}_min\"),\n",
    "                    max(col(col_name)).alias(f\"{col_name}_max\")\n",
    "                ).writeStream \\\n",
    "                 .outputMode(\"complete\") \\\n",
    "                 .format(\"console\") \\\n",
    "                 .start()\n",
    "\n",
    "                numeric_summary_query.awaitTermination()\n",
    "\n",
    "        total_count_query.awaitTermination()\n",
    "\n",
    "    except StreamingQueryException as e:\n",
    "        print(f\"Error in streaming query: {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"ValueError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "550ea472-b001-4199-bc33-f63b70ae9cee",
     "showTitle": true,
     "title": "FN: Transformation Functions for Customers dataframe"
    }
   },
   "outputs": [],
   "source": [
    "# FN: Transformation Functions for Customers dataframe\n",
    "\n",
    "# 1. Formating phone number\n",
    "def format_phone_numbers(df: DataFrame, phone_column: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Converts a phone number column to a numerical column by removing non-numeric characters.\n",
    "    \"\"\"\n",
    "    # Remove non-numeric characters and cast to LongType\n",
    "    df = df.withColumn(phone_column, regexp_replace(col(phone_column), \"[^0-9]\", \"\").cast(LongType()))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 2. Domain extraction\n",
    "def domain_extract(df : DataFrame, col_name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts the domain from the email address column and adds it as a new column.\n",
    "    \"\"\"\n",
    "    df = df.withColumn(\"domain\", split(col(col_name), \"@\")[1])\n",
    "    return df\n",
    "\n",
    "\n",
    "# 3. Postal code extraction\n",
    "def extract_pincode(df: DataFrame, address_column: str, pincode_column: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts the pincode (postal code) from the end of an address column and adds it as a new column.\n",
    "    \"\"\"\n",
    "    # Extract the pincode (digits at the end of the address string)\n",
    "    df = df.withColumn(pincode_column, regexp_extract(col(address_column), r'(\\d+)$', 1).cast(IntegerType()))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88f446c1-7efd-49cd-9383-3fa2ba734651",
     "showTitle": true,
     "title": "FN: Conversion Rate function"
    }
   },
   "outputs": [],
   "source": [
    "# FN: Conversion Rate function\n",
    "def fetch_conversion_rates(base_currency='USD'):\n",
    "    \"\"\"\n",
    "    Fetches conversion rates for a given base currency from ExchangeRate-API.\n",
    "    \"\"\"\n",
    "    url = f'https://api.exchangerate-api.com/v4/latest/{base_currency}'\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        data = response.json()\n",
    "        rates = data.get('rates', {})\n",
    "        return rates\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching conversion rates: {e}\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d213794d-0ed1-4c37-96fa-c75716334fbb",
     "showTitle": true,
     "title": "FN: Converting all amount to USD"
    }
   },
   "outputs": [],
   "source": [
    "# FN: Converting all amount to USD\n",
    "def convert_currency_to_usd(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Converts currency amounts in a DataFrame to USD.\n",
    "    \"\"\"\n",
    "\n",
    "    # conversion Rates\n",
    "    conversion_rates = fetch_conversion_rates()\n",
    "    \n",
    "    # Broadcast the conversion rates dictionary\n",
    "    broadcasted_conversion_rates = df.sql_ctx.sparkSession.sparkContext.broadcast(conversion_rates)\n",
    "\n",
    "    # Define the UDF for currency conversion\n",
    "    def convert_currency(amount, currency):\n",
    "        if amount is None:\n",
    "            return None  # Return None if amount is None\n",
    "        rate = broadcasted_conversion_rates.value.get(currency, 1)  # Default to 1 if currency not found\n",
    "        return amount / rate\n",
    "\n",
    "    # Register the UDF\n",
    "    convert_currency_udf = udf(convert_currency, DoubleType())\n",
    "\n",
    "    # Apply the UDF to create the 'Amount_USD' column\n",
    "    transformed_df = df.withColumn(\"Amount_USD\", convert_currency_udf(col(\"amount\"), col(\"currency\")))\n",
    "\n",
    "    return transformed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5012ccac-ceac-4a0d-8daa-ac7c08ad4886",
     "showTitle": true,
     "title": "FN: Getting Coordinate for locations"
    }
   },
   "outputs": [],
   "source": [
    "# FN: Getting Coordinate for locations\n",
    "def get_coordinates(location: str) -> list:\n",
    "    \"\"\"\n",
    "    Fetches coordinates for a given location using the Nominatim API.\n",
    "    \"\"\"\n",
    "    time.sleep(2)\n",
    "    # handling spaces in the location names\n",
    "    if \" \" in location:\n",
    "        location = location.replace(\" \",\"_\")\n",
    "\n",
    "    encoded_location = quote(location)  # URL encode the location\n",
    "    url = f'https://nominatim.openstreetmap.org/search?q={encoded_location}&format=json&addressdetails=1'\n",
    "    max_retries = 3\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "            data = response.json()\n",
    "            if data:\n",
    "                lat = data[0]['lat']\n",
    "                lng = data[0]['lon']\n",
    "                return [float(lat), float(lng)]\n",
    "            else:\n",
    "                return [None, None]\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            # Exponential backoff\n",
    "            time.sleep(2 ** attempt+1)  \n",
    "    \n",
    "    return [None, None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6320cbf2-94ca-487a-99ac-7b3a05ee3442",
     "showTitle": true,
     "title": "gold Functions"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b477db93-0b62-4c0f-80da-e1a314870c72",
     "showTitle": true,
     "title": "FN: Joining all tables"
    }
   },
   "outputs": [],
   "source": [
    "# FN: Joining all tables\n",
    "def join_customer_branch_transaction(customers_df, branches_df, transactions_df):\n",
    "    # Alias the DataFrames to handle columns with the same name\n",
    "    customers_df = customers_df.alias(\"cust\")\n",
    "    branches_df = branches_df.alias(\"branch\")\n",
    "    transactions_df = transactions_df.alias(\"txn\")\n",
    "\n",
    "    # Perform inner join between transactions and customers on customer_id\n",
    "    transactions_customers_df = transactions_df.join(customers_df, on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "    # Perform inner join between the above result and branches on branch_id\n",
    "    full_join_df = transactions_customers_df.join(branches_df, on=\"branch_id\", how=\"inner\")\n",
    "\n",
    "    # List of columns to drop from customers and branches (except the join keys)\n",
    "    columns_to_drop = [f\"cust.{col}\" for col in customers_df.columns if col != \"customer_id\"] + \\\n",
    "                      [f\"branch.{col}\" for col in branches_df.columns if col != \"branch_id\"]\n",
    "\n",
    "    # Drop the columns from the joined DataFrame\n",
    "    full_join_df = full_join_df.drop(*columns_to_drop)\n",
    "\n",
    "    return full_join_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9c2cfdd-ba44-453c-b098-7e23e192ad3a",
     "showTitle": true,
     "title": "FN: Aggregating customer data"
    }
   },
   "outputs": [],
   "source": [
    "# FN: Aggregating customer data\n",
    "def aggregate_customers(customers_df, transactions_df):\n",
    "    # Join customers_df with transactions_df on customer_id\n",
    "    joined_df = customers_df.join(transactions_df, \"customer_id\")\n",
    "\n",
    "    # Customer Segmentation\n",
    "    customer_segmentation = joined_df.groupBy(\"customer_type\") \\\n",
    "        .agg(\n",
    "            F.countDistinct(\"customer_id\").alias(\"total_customers\"),\n",
    "            F.avg(\"amount\").alias(\"avg_transaction_amount\"),\n",
    "            F.sum(\"amount\").alias(\"total_spend\")\n",
    "        )\n",
    "\n",
    "    # Find the latest transaction timestamp\n",
    "    last_transaction_date = transactions_df.select(F.max(\"timestamp\")).collect()[0][0]\n",
    "\n",
    "    # Define a window to calculate the most recent transaction date for each customer\n",
    "    window_spec = Window.partitionBy(\"customer_id\")\n",
    "\n",
    "    # Add a column with the most recent transaction date for each customer\n",
    "    transactions_df = transactions_df.withColumn(\n",
    "        \"last_transaction_date\", F.max(\"timestamp\").over(window_spec)\n",
    "    )\n",
    "\n",
    "    # Identify active customers: transactions within the last 180 days\n",
    "    active_customers_df = transactions_df.filter(\n",
    "        F.datediff(F.lit(last_transaction_date), \"last_transaction_date\") <= 180\n",
    "    ).select(\"customer_id\").distinct()\n",
    "\n",
    "    # Identify churned customers: no transactions in the last 365 days\n",
    "    churned_customers_df = transactions_df.filter(\n",
    "        F.datediff(F.lit(last_transaction_date), \"last_transaction_date\") > 180\n",
    "    ).select(\"customer_id\").distinct()\n",
    "\n",
    "    # Get the counts\n",
    "    active_customers = active_customers_df.count()\n",
    "    churned_customers = churned_customers_df.count()\n",
    "\n",
    "    return customer_segmentation, active_customers, churned_customers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f843455e-8220-4a81-b353-456087434e4e",
     "showTitle": true,
     "title": "FN: Branch Performance and Analysis"
    }
   },
   "outputs": [],
   "source": [
    "# FN: Branch Performance and Analysis\n",
    "def Currency_Analysis(transactions_df):\n",
    "      # Currency Analysis\n",
    "    currency_analysis = transactions_df.groupBy(\"currency\") \\\n",
    "        .agg(\n",
    "            F.sum(\"Amount_USD\").alias(\"total_amount_by_currency\"),\n",
    "            F.avg(\"Amount_USD\").alias(\"avg_amount_by_currency\")\n",
    "        )\n",
    "\n",
    "    return currency_analysis\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GB_Project_Config",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
